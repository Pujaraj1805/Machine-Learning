---
title: "Untitled"
author: "Puja Raj"
date: "10/22/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Machine learning could truly mark its place in today's society because of the availability of data. With data the generalisation of new data is possible. THis discovery led to many inventions like FitBit,Nike Fuel Band and so on. They basically cllect data on personal activity in large amounts. It is often used to encourage individuals who want to track their exercise in day to improve their health or want to know about a routine that is best suited to them. This data is analysed to infer knowledge from teh data.

For this project, we will be usng data recordd from accelerometrs on the belt, forearm, arm, and dumbbell of 6 participants.

## Context
We are considering 5 ways to lift barbells(A,B,c,D,E). The participants are asked to perform all teh 5 in correct and incorrect ways.


The analysis is done to predict the kind of barbell lift exercise(A,B,C,D,E) 


Please find more info  which is available from the website http://groupware.les.inf.puc-rio.br/har 

Large amounts of data in their raw form can be very intmidating. We will analyse the data collected 

#Sources for collected data


Data for training: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


Data for testing:  https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


## Installling Libraries
Libraries are installed in one chunk for reference later and the seed is set.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
    library(randomForest)
    library(rpart)
    library(ggplot2)
    library(caret)
    library(gbm)
    library(plyr)
    set.seed(17790869)
```

## Loading & Cleaning Data
Note: The data was previously downloaded from source, it is not included in repository.
We observe the data in RStudio and then load the data into file removing empty elements.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
    training <- read.csv("./training-ml.csv", na.strings=c("NA","#DIV/0!",""))
    testing <- read.csv("./testing-ml.csv", na.strings=c("NA","#DIV/0!",""))
```

## Structuring the Dataset
For this model, we only need a limited set of information as discussed in the Introduction, thus we isolate the data from the source with only the information we need. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
    features <- names(testing[,colSums(is.na(testing)) == 0])[8:59]
    training <- training[,c(features,"classe")]
    testing <- testing[,c(features,"problem_id")]
```

## Seperating the Dev and Train set out of training data
As our test set exists separately, we have to separate our training data into train-set and dev-set. As a rule of thumb we're going to be testing a 60:40 split, and 80:20 split. The 80:20 split performed marginally better and hence is the focus of this project report.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
inTrain <- createDataPartition(training$classe, p=0.8, list=FALSE)
trainSet <- training[inTrain,]
devSet <- training[-inTrain,]
```

## Building the Decision Tree
We build a Decision Tree Model on the base data set

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(rattle)
training_tree <- rpart(classe ~ ., data = trainSet, 
                       method="class", 
                       control = rpart.control(method = "cv", number = 10))
fancyRpartPlot(training_tree)
```

## Predicting with the Decision Tree
We predict the outputs using this Decision Tree Model, we don't expect a good accuracy with the model just yet cause of the out of bound error possibilities. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
prediction <- predict(training_tree, devSet, type = "class")
classe <- as.factor(devSet$classe)
confusionMatrix(prediction, classe)
```

From the above table we see that the accuracy of the Decision Tree is around 72.24%. This is a low accuracy, we now try to improve the accuracy by trying out a Random Forest Model.

## Building Random Forest Model
Now, we build a random forest model

```{r, echo=TRUE, message=FALSE, warning=FALSE}
randomForestdata <- randomForest(as.factor(classe) ~ ., data = trainSet, 
                   method = "rf", 
                   importance = T, 
                   trControl = trainControl(method = "cv", 
                                            classProbs=TRUE, 
                                            savePredictions=TRUE,
                                            allowParallel=TRUE,
                                            number = 11))
plot(randomForestdata)
```

## Predicting with Random Forest
When the Random forests are created, the model itself carves out a portion of data to avoid out of sample errors, and thus gives a better accuracy.

Let's try and predict the outcomes now and see the accuracy of the same.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
prediction <- predict(randomForestdata, devSet, type = "class")
classe <- as.factor(devSet$classe)
confusionMatrix(prediction, classe)
```

From the confusion matrix we see that the stray data cases are very less and the accuracy of Random Forest Model is around 99.34%. Hence, it would be better than Decision Tree Model to use for predicting the values for the testing data (_pml-testing.csv_).

## Predicting on the Testing Data
```{r, echo=TRUE, message=FALSE, warning=FALSE}
prediction <- predict(randomForestdata, training, type = "class")
classe <- as.factor(training$classe)
confusionMatrix(prediction, classe)
```

Thus we see an accuracy of 99.87% on our Testing data set(_pml-testing.csv_).